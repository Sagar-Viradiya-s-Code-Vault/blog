<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Vibe coding offline with local LLM and JetBrains AI | Sagar's Blog</title>
<meta name=keywords content="AI,LLM,JetBarains AI,Vibe Coding"><meta name=description content="A guide to setup local LLM."><meta name=author content><link rel=canonical href=https://sagarviradiya.dev/posts/vibe-coding-offline/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.0346ff196149e1e8907ccd5dda7af0e63f0dea21feb0e2c4d5853b9e6554dca3.css integrity="sha256-A0b/GWFJ4eiQfM1d2nrw5j8N6iH+sOLE1YU7nmVU3KM=" rel="preload stylesheet" as=style><link rel=icon href=https://sagarviradiya.dev/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://sagarviradiya.dev/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://sagarviradiya.dev/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://sagarviradiya.dev/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://sagarviradiya.dev/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://sagarviradiya.dev/posts/vibe-coding-offline/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><meta property="og:url" content="https://sagarviradiya.dev/posts/vibe-coding-offline/"><meta property="og:site_name" content="Sagar's Blog"><meta property="og:title" content="Vibe coding offline with local LLM and JetBrains AI"><meta property="og:description" content="A guide to setup local LLM."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-05-08T00:00:00+00:00"><meta property="article:modified_time" content="2025-05-08T00:00:00+00:00"><meta property="article:tag" content="AI"><meta property="article:tag" content="LLM"><meta property="article:tag" content="JetBarains AI"><meta property="article:tag" content="Vibe Coding"><meta property="og:image" content="https://sagarviradiya.dev/images/header.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://sagarviradiya.dev/images/header.png"><meta name=twitter:title content="Vibe coding offline with local LLM and JetBrains AI"><meta name=twitter:description content="A guide to setup local LLM."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://sagarviradiya.dev/posts/"},{"@type":"ListItem","position":2,"name":"Vibe coding offline with local LLM and JetBrains AI","item":"https://sagarviradiya.dev/posts/vibe-coding-offline/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Vibe coding offline with local LLM and JetBrains AI","name":"Vibe coding offline with local LLM and JetBrains AI","description":"A guide to setup local LLM.","keywords":["AI","LLM","JetBarains AI","Vibe Coding"],"articleBody":"With most popular IDEs now integrating AI tools, AI has become an integral part of the development process. Vibe coding helps you to iterate or fix something faster, making you more productive! While it is fun to vibe code, there is also a concern on privacy. Especially if you are sharing context (Proprietary code snippet or giving access to the entire codebase). For closed-source code, you have to be very careful how you are using AI and what you are sharing with third-party models. For open source code or a quick POC, you can go full brave mode and explore everything that the AI tool can offer.\nSome AI tools, like Gemini integration within Android Studio, allow you to use them without sharing context. But the answers you get won‚Äôt be as clear and crisp as they would be if the model had full context.\nLately, JetBrains announced its AI integration to all IDEs. You can choose among many third-party models like GPT-4o, Gemini 2.5 pro, etc that will be used along with JetBrains proprietary model. Like Gemini in Android Studio, you can also disable context and use JetBrains AI like another chat assistant. What caught my eye, though, was the ability to plug in your local LLM.\nPlugging local LLM means you can share context without worrying about privacy. This blog is my exploration of vibe coding offline by plugging a local LLM to JetBrains AI. I will show you how you can spin your local LLM and connect with JetBrains AI, and what the caveats are with local LLM.\nRunning local LLM ‚ú® First things first, how can you run a local LLM? There are different ways you can run LLM on your machine. You can use GUI based tool like LM Studio, there is also Ollama, which is a CLI tool. IntelliJ AI offers connecting LLM through both LM Studio and Ollama. Since I am new to AI game I chose the LM studio as I found it easy and they have good documentation. Let‚Äôs see how you can spin up LLM using LM Studio.\nSince each model has different capabilities and some are trained and optimized for a specific task, you need to decide which model is best for you. You can browse through different models available on Hugging Face ü§ó.\nAfter installing LM Studio, you can go to the Discover tab to browse through all the models available.\nOnce the model is downloaded, you need to load it.\nThen select the downloaded model. Once it is up and running, you can start chatting with LLM in the LM studio.\nYou just created an AI chat assistant backed by LLM running locally!\nPlugging local LLM to JetBrains AI üîå Now we have to connect LLM to JetBrains AI. Go to the Developer Tab and start a server on localhost. This will allow plugging LLM into the third-party tool. In our case, it is IntelliJ.\nOnce the server is up, it will show you on which IP and port it is reachable. You need this while connecting to the server within IntelliJ. In the above screenshot, you can see server is reachable at http://127.0.0.1:1234\nLet‚Äôs switch to IntelliJ and see how we can connect LLM.\nEnable LM Studio within third-party providers and enter the URL on which localhost is running. If everything is good, you should see a green tick mark after you enter the URL and click Test Connection.\nAlong with thrid-party AI provider, you also need to configure local models and select one running on your machine for Core features and Instant helpers. In the above screenshot, I have chosen the DeepSeek model.\nFinally, enable offline mode, which does not guarantee 100% offline mode, as you can see from the following warning.\nPrevents most remote calls, prioritizing local models. Despite these safeguards, rare instances of cloud usage may still occur.\nFor this, make sure you disconnect your machine from the internet if you are working on a proprietary project.\nVibe coding offline ü¶ñ You are the T-REX now and ready to vibe code offline! Once you configure the local model, you should be able to select it from the chat assistant.\nIn the screenshot above, I selected the deepseek-coder-v2-lite-instruct-mlx model. I found this model to be fast and accurate, answering all Kotlin-related questions. But your mileage may vary based on your hardware and the type of questions you want to ask. I would say do your research ask AI which model is best for your use case üòõ\nAnother thing to notice in the above screenshot is that I have turned off the automatic context attachment of Codebase. I found that for few queries it didn‚Äôt pick the right files for context. You can manually specify files by dragging files to the chat input section for precise control over context.\nLet‚Äôs try the above setup and see if it is working. I was exploring KotlinConf official KMP app codebase and asked few questions.\nFirst, I wanted to know what all screens are there in the app, so I just dropped the KotlinConfNavHost file.\nYour browser does not support the video tag. I would say the answer I got was quite satisfactory. This one was simple and not even AI worth question. I just wanted to test if it can answer easy questions.\nNext, I wanted to know the logic behind local notification scheduling logic.\nYour browser does not support the video tag. Again, I was happy with the result. It broke down the function line by line, explaining the logic.\nLastly, I asked about the iOS implementation of the notification service logic.\nYour browser does not support the video tag. The answer was precise and crisp. After this, I asked a few basic questions related to kotlin in general, and it was able to answer them correctly. The local LLM was up and running, really pushing my Apple Silicon to its limits! However, local LLM has some downsides. Let‚Äôs discuss them.\nCaveats with local LLM 1. Passing context Auto inference of the context didn‚Äôt work as expected for me. It was hit or miss. After turning on Codebase context in the chat input and asking a generic question, for example, ‚ÄúCould you please explain this file for me?‚Äù, it didn‚Äôt pick the open file but some other random files. Attaching the file manually worked best for me.\nAnother issue was, unlike third-party LLMs running on the cloud, local LLMs have limitations on the size of the context. So, if you want to attach the entire package containing related files, it is not possible.\n2. Lack of generic inference Due to the limitation on context auto-inference that I mentioned above, the answer to super generic question, for example, ‚ÄúBreak down this project for me?‚Äù won‚Äôt be possible. Since you need to manually attach context, and there is a size limitation, it is super hard to get answer to such question.\nBottom line is, local LLM approach is best if you have some knowledge about the project and want to ask questions specific to files or code snippets.\nParting thoughts So far, my exploration to local LLM is very limited, but at least the initial result is promising. Local LLMs are great for closed-source projects where you need to delegate everyday tasks to AI. Also, imagine you have limited or no internet access, you can still get help from AI. As local models continue to improve and hardware gets better and better, running local LLM will become the norm.\nJetBrains AI is in the initial phase, and I hope that support for local LLM and all the limitations will improve in the upcoming iterations. If you‚Äôre on the fence, I highly recommend giving it a try. If you‚Äôve tried running local LLMs, I‚Äôd love to hear your approach and thoughts in the comments below.\nUntil next time! Stay curious ‚úåüèª\n","wordCount":"1313","inLanguage":"en","image":"https://sagarviradiya.dev/images/header.png","datePublished":"2025-05-08T00:00:00Z","dateModified":"2025-05-08T00:00:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://sagarviradiya.dev/posts/vibe-coding-offline/"},"publisher":{"@type":"Organization","name":"Sagar's Blog","logo":{"@type":"ImageObject","url":"https://sagarviradiya.dev/%3Clink%20/%20abs%20url%3E"}}}</script></head><body class=dark id=top><script>localStorage.getItem("pref-theme")==="light"&&document.body.classList.remove("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://sagarviradiya.dev/ accesskey=h title="Sagar Viradiya (Alt + H)"><img src=https://sagarviradiya.dev/apple-touch-icon.png alt aria-label=logo height=35>Sagar Viradiya</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://sagarviradiya.dev/posts/ title=Posts><span>Posts</span></a></li><li><a href=https://sagarviradiya.dev/talks/ title=Talks><span>Talks</span></a></li><li><a href=https://sagarviradiya.dev/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://sagarviradiya.dev/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://sagarviradiya.dev/>Home</a>&nbsp;¬ª&nbsp;<a href=https://sagarviradiya.dev/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Vibe coding offline with local LLM and JetBrains AI</h1><div class=post-meta><span title='2025-05-08 00:00:00 +0000 UTC'>May 8, 2025</span>&nbsp;¬∑&nbsp;7 min&nbsp;¬∑&nbsp;1313 words</div></header><figure class=entry-cover><a href=https://sagarviradiya.dev/posts/vibe-coding-offline/images/header.png target=_blank rel="noopener noreferrer"><img loading=eager srcset='https://sagarviradiya.dev/posts/vibe-coding-offline/images/header_hu_7e5cae208515b0f7.png 360w,https://sagarviradiya.dev/posts/vibe-coding-offline/images/header_hu_cc595d5ea79b9bc9.png 480w,https://sagarviradiya.dev/posts/vibe-coding-offline/images/header_hu_30760aabb8e47f0f.png 720w,https://sagarviradiya.dev/posts/vibe-coding-offline/images/header_hu_ee20d33af580f05.png 1080w,https://sagarviradiya.dev/posts/vibe-coding-offline/images/header_hu_d354add282bc6fa1.png 1500w,https://sagarviradiya.dev/posts/vibe-coding-offline/images/header.png 1536w' src=https://sagarviradiya.dev/posts/vibe-coding-offline/images/header.png sizes="(min-width: 768px) 720px, 100vw" width=1536 height=1024 alt="Image generated with the help of AI (ChatGPT/DALL¬∑E)"></a><figcaption>Image generated with the help of AI (ChatGPT/DALL¬∑E)</figcaption></figure><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#running-local-llm->Running local LLM ‚ú®</a></li><li><a href=#plugging-local-llm-to-jetbrains-ai->Plugging local LLM to JetBrains AI üîå</a></li><li><a href=#vibe-coding-offline->Vibe coding offline ü¶ñ</a></li><li><a href=#caveats-with-local-llm>Caveats with local LLM</a><ul><li><a href=#1-passing-context>1. Passing context</a></li><li><a href=#2-lack-of-generic-inference>2. Lack of generic inference</a></li></ul></li><li><a href=#parting-thoughts>Parting thoughts</a></li></ul></nav></div></details></div><div class=post-content><p>With most popular IDEs now integrating AI tools, AI has become an integral part of the development process. Vibe coding helps you to iterate or fix something faster, making you more productive! While it is fun to vibe code, there is also a concern on privacy. Especially if you are sharing context (Proprietary code snippet or giving access to the entire codebase). For closed-source code, you have to be very careful how you are using AI and what you are sharing with third-party models. For open source code or a quick POC, you can go full brave mode and explore everything that the AI tool can offer.</p><p>Some AI tools, like Gemini integration within Android Studio, allow you to use them without sharing context. But the answers you get won‚Äôt be as clear and crisp as they would be if the model had full context.</p><p>Lately, JetBrains announced its AI integration to all IDEs. You can choose among many third-party models like GPT-4o, Gemini 2.5 pro, etc that will be used along with JetBrains proprietary model. Like Gemini in Android Studio, you can also disable context and use JetBrains AI like another chat assistant. What caught my eye, though, was the ability to plug in your local LLM.</p><p>Plugging local LLM means you can share context without worrying about privacy. This blog is my exploration of vibe coding offline by plugging a local LLM to JetBrains AI. I will show you how you can spin your local LLM and connect with JetBrains AI, and what the caveats are with local LLM.</p><h2 id=running-local-llm->Running local LLM ‚ú®<a hidden class=anchor aria-hidden=true href=#running-local-llm->#</a></h2><p>First things first, how can you run a local LLM? There are different ways you can run LLM on your machine. You can use GUI based tool like <a href=https://lmstudio.ai>LM Studio</a>, there is also Ollama, which is a CLI tool. IntelliJ AI offers connecting LLM through both <a href=https://lmstudio.ai>LM Studio</a> and Ollama. Since I am new to AI game I chose the <a href=https://lmstudio.ai>LM studio</a> as I found it easy and they have good documentation. Let&rsquo;s see how you can spin up LLM using <a href=https://lmstudio.ai>LM Studio</a>.</p><p>Since each model has different capabilities and some are trained and optimized for a specific task, you need to decide which model is best for you. You can browse through different models available on <a href=https://huggingface.co>Hugging Face ü§ó</a>.</p><p>After installing <a href=https://lmstudio.ai>LM Studio</a>, you can go to the Discover tab to browse through all the models available.</p><figure><img loading=lazy src=images/lm_studio_home.png></figure><figure><img loading=lazy src=images/browsing_model.png></figure><p>Once the model is downloaded, you need to load it.</p><figure><img loading=lazy src=images/select_load_model.png></figure><p>Then select the downloaded model. Once it is up and running, you can start chatting with LLM in the LM studio.</p><figure><img loading=lazy src=images/lm_studio_running_llm.png></figure><p>You just created an AI chat assistant backed by LLM running locally!</p><h2 id=plugging-local-llm-to-jetbrains-ai->Plugging local LLM to JetBrains AI üîå<a hidden class=anchor aria-hidden=true href=#plugging-local-llm-to-jetbrains-ai->#</a></h2><p>Now we have to connect LLM to JetBrains AI. Go to the Developer Tab and start a server on localhost. This will allow plugging LLM into the third-party tool. In our case, it is IntelliJ.</p><figure><img loading=lazy src=images/lm_studio_developer_tab.png></figure><figure><img loading=lazy src=images/lm_studio_server_running.png></figure><p>Once the server is up, it will show you on which IP and port it is reachable. You need this while connecting to the server within IntelliJ. In the above screenshot, you can see server is reachable at <code>http://127.0.0.1:1234</code></p><p>Let&rsquo;s switch to IntelliJ and see how we can connect LLM.</p><figure><img loading=lazy src=images/intelliJ_connect_local_llm.png></figure><p>Enable LM Studio within third-party providers and enter the URL on which localhost is running. If everything is good, you should see a green tick mark after you enter the URL and click Test Connection.</p><p>Along with thrid-party AI provider, you also need to configure local models and select one running on your machine for Core features and Instant helpers. In the above screenshot, I have chosen the DeepSeek model.</p><p>Finally, enable offline mode, which does not guarantee 100% offline mode, as you can see from the following warning.</p><blockquote><p>Prevents most remote calls, prioritizing local models. Despite these safeguards, rare instances of cloud usage may still occur.</p></blockquote><p>For this, make sure you disconnect your machine from the internet if you are working on a proprietary project.</p><h2 id=vibe-coding-offline->Vibe coding offline ü¶ñ<a hidden class=anchor aria-hidden=true href=#vibe-coding-offline->#</a></h2><p>You are the T-REX now and ready to vibe code offline! Once you configure the local model, you should be able to select it from the chat assistant.</p><figure><img loading=lazy src=images/intellij_select_local_llm.png></figure><p>In the screenshot above, I selected the <code>deepseek-coder-v2-lite-instruct-mlx</code> model. I found this model to be fast and accurate, answering all Kotlin-related questions. But your mileage may vary based on your hardware and the type of questions you want to ask. I would say <del>do your research</del> ask AI which model is best for your use case üòõ</p><p>Another thing to notice in the above screenshot is that I have turned off the automatic context attachment of Codebase. I found that for few queries it didn&rsquo;t pick the right files for context. You can manually specify files by dragging files to the chat input section for precise control over context.</p><p>Let&rsquo;s try the above setup and see if it is working. I was exploring KotlinConf official <a href=https://github.com/JetBrains/kotlinconf-app>KMP app</a> codebase and asked few questions.</p><p>First, I wanted to know what all screens are there in the app, so I just dropped the <code>KotlinConfNavHost</code> file.</p><video controls autoplay loop muted style=max-width:100%;height:auto>
<source src=/videos/local_llm_demo.mp4 type=video/mp4>Your browser does not support the video tag.</video><p>I would say the answer I got was quite satisfactory. This one was simple and not even AI worth question. I just wanted to test if it can answer easy questions.</p><p>Next, I wanted to know the logic behind local notification scheduling logic.</p><video controls autoplay loop muted style=max-width:100%;height:auto>
<source src=/videos/local_llm_demo_2.mp4 type=video/mp4>Your browser does not support the video tag.</video><p>Again, I was happy with the result. It broke down the function line by line, explaining the logic.</p><p>Lastly, I asked about the iOS implementation of the notification service logic.</p><video controls autoplay loop muted style=max-width:100%;height:auto>
<source src=/videos/local_llm_demo_3.mp4 type=video/mp4>Your browser does not support the video tag.</video><p>The answer was precise and crisp. After this, I asked a few basic questions related to kotlin in general, and it was able to answer them correctly. The local LLM was up and running, really pushing my Apple Silicon to its limits! However, local LLM has some downsides. Let&rsquo;s discuss them.</p><h2 id=caveats-with-local-llm>Caveats with local LLM<a hidden class=anchor aria-hidden=true href=#caveats-with-local-llm>#</a></h2><h3 id=1-passing-context>1. Passing context<a hidden class=anchor aria-hidden=true href=#1-passing-context>#</a></h3><p>Auto inference of the context didn&rsquo;t work as expected for me. It was hit or miss. After turning on <code>Codebase</code> context in the chat input and asking a generic question, for example, &ldquo;Could you please explain this file for me?&rdquo;, it didn&rsquo;t pick the open file but some other random files. Attaching the file manually worked best for me.</p><p>Another issue was, unlike third-party LLMs running on the cloud, local LLMs have limitations on the size of the context. So, if you want to attach the entire package containing related files, it is not possible.</p><h3 id=2-lack-of-generic-inference>2. Lack of generic inference<a hidden class=anchor aria-hidden=true href=#2-lack-of-generic-inference>#</a></h3><p>Due to the limitation on context auto-inference that I mentioned above, the answer to
super generic question, for example, &ldquo;Break down this project for me?&rdquo; won&rsquo;t be possible. Since you need to manually attach context, and there is a size limitation, it is super hard to get answer to such question.</p><p>Bottom line is, local LLM approach is best if you have some knowledge about the project and want to ask questions specific to files or code snippets.</p><h2 id=parting-thoughts>Parting thoughts<a hidden class=anchor aria-hidden=true href=#parting-thoughts>#</a></h2><p>So far, my exploration to local LLM is very limited, but at least the initial result is promising. Local LLMs are great for closed-source projects where you need to delegate everyday tasks to AI. Also, imagine you have limited or no internet access, you can still get help from AI. As local models continue to improve and hardware gets better and better, running local LLM will become the norm.</p><p>JetBrains AI is in the initial phase, and I hope that support for local LLM and all the limitations will improve in the upcoming iterations. If you‚Äôre on the fence, I highly recommend giving it a try. If you‚Äôve tried running local LLMs, I‚Äôd love to hear your approach and thoughts in the comments below.</p><p>Until next time! Stay curious ‚úåüèª</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://sagarviradiya.dev/tags/ai/>AI</a></li><li><a href=https://sagarviradiya.dev/tags/llm/>LLM</a></li><li><a href=https://sagarviradiya.dev/tags/jetbarains-ai/>JetBarains AI</a></li><li><a href=https://sagarviradiya.dev/tags/vibe-coding/>Vibe Coding</a></li></ul><nav class=paginav><a class=next href=https://sagarviradiya.dev/posts/compose-animation-part-03/><span class=title>Next ¬ª</span><br><span>Compose Animation, Under The Hood - Part III</span></a></nav></footer><div id=disqus_thread></div><script>window.disqus_config=function(){},function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById("disqus_thread").innerHTML="Disqus comments not available by default when the website is previewed locally.";return}var t=document,e=t.createElement("script");e.async=!0,e.src="//sagarviradiya-dev.disqus.com/embed.js",e.setAttribute("data-timestamp",+new Date),(t.head||t.body).appendChild(e)}()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></article></main><footer class=footer><span>&copy; 2025 <a href=https://sagarviradiya.dev/>Sagar's Blog</a></span> ¬∑
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>